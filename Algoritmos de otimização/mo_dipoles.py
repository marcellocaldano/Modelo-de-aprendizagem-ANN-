# -*- coding: utf-8 -*-
"""mo-dipolo2OBJ-V2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FxP3VmlnKoF87mpKUgzrQNz-BJE0v3VN

# Multi-Objetctive Experiments | Base Dipolo Simples

In this Jupyter Notebook we will show you how to define from scrath a MOO (Multi Objective Optimization Problem) that is modeled from a previous trained regression model.

We will be using the JMetalPy library for the MO Optimization and the standard SKlearn Library and PyData Libraries for Machine Learning

### Tables of Contents
<a id='top'></a>

1. [Loading Essential Libraries](#loading)
2. [Loading JMetal Libraries](#jmetal)
3. [Loading Database for Regression](#data)
    1. [Data Visualization](#visu)
4. [Training a Multitarget Regression Model](#regression)
    1. [Helper Functions](#helper)
    2. [GridSearch with Cross Validation](#grid)
    3. [Visualizing output of target Variables](#target)
5. [Exporting and loading the model with pickle](#pickle)
6. [Definig our own MOO Problem with JMetalPy Inheritance](#problem)
7. [Running a Instance of our MOO Problem](#run)
   1. [Redefining the Algorithms](#re_impl)<br>
   2. [Running the Algorithms](#re_run)<br>
   3. [Front Analysis of Algorithms](#enha_front)<br>
   4. [Front Overllapin Comparison](#over_visu)<br>
   5. [Front Convergence](#front_conv)<br>
   6. [Normalized Front Convergence Comparison](#normal_front_conv)<br>

### Loading Essential Libraries
<a id='loading'></a>

[Back to top](#top)
"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

from sklearn.svm import SVR
from sklearn.neural_network import MLPRegressor

from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import KFold
from sklearn.model_selection import GridSearchCV

from sklearn.metrics import mean_absolute_error
from sklearn.metrics import mean_squared_error

from sklearn.multioutput import MultiOutputRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import MinMaxScaler

from sklearn.pipeline import Pipeline
from IPython.display import display

"""### Loading JMETAL Library
<a id='jmetal'></a>

[Back to top](#top)
"""

import os
import pickle

from jmetal.core.problem import FloatProblem
from jmetal.core.solution import FloatSolution

from jmetal.algorithm.multiobjective import smpso
from jmetal.algorithm.multiobjective import nsgaii
from jmetal.algorithm.multiobjective import omopso
from jmetal.algorithm.multiobjective import spea2
from jmetal.algorithm.multiobjective import moead
from jmetal.algorithm.multiobjective import ibea

from jmetal.operator import SBXCrossover
from jmetal.operator import PolynomialMutation
from jmetal.operator import DifferentialEvolutionCrossover

from jmetal.util.aggregative_function import Tschebycheff

from jmetal.util.termination_criterion import StoppingByEvaluations
from jmetal.util.termination_criterion import QualityIndicator

from jmetal.util.archive import CrowdingDistanceArchive

from jmetal.operator import UniformMutation
from jmetal.operator.mutation import NonUniformMutation

"""### Loading Database for Regression

[Back to top](#top)
"""

from IPython.display import display

database_dipole_bw = pd.read_excel('data/BW_SMA_database.xlsx')
display(database_dipole_bw.head())

pd.read_excel('data/dipolo_f-bw.xlsx')

print(len(database_dipole_bw))

plt.figure(figsize=(10, 8))
corr = database_dipole_bw.corr()
sns.heatmap(corr, annot=True, cbar=False, cmap='Reds')

"""#### Data Visualization
<a id='visu'></a>
[Back to top](#top)
"""

fig, axis = plt.subplots(nrows=1, ncols=2, figsize=(20, 7))
axis = axis.ravel()

sns.distplot(database_dipole_bw['BW'], ax=axis[0])
axis[0].set_title('Distribuicao de BW na base Dipole-BW')

sns.distplot(database_dipole_bw['f'], ax=axis[1])
axis[1].set_title('Distribuicao de FCentral na base Dipole-BW')

"""### Training a Multitarget Regression Model

[Back to top](#top)

#### Helper Functions
<a id='helper'></a>

[Back to top](#top)

Function that will perform the K-Fold Cross Validation of a regressor model, and gives as an output the best estimator and the splited test data, so we can compute the metrics and plot the graphs
"""

def experiment_train_kfold(data, estimator, param_grid, targets, metric='r2', normalize_target=False):
    
    X = data.drop(labels=targets, axis=1)
    Y = data[targets]
    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)

    std_in = StandardScaler()
    X_train = std_in.fit_transform(X_train)
    X_test = std_in.transform(X_test)

    std_out = None
    if normalize_target:
        std_out = StandardScaler()
        Y_train = std_out.fit_transform(Y_train)
        Y_test = std_out.transform(Y_test)

    grid = GridSearchCV(estimator=estimator, 
                        param_grid=param_grid,
                        scoring=metric, 
                        cv=KFold(n_splits=10), 
                        verbose=1, 
                        n_jobs=-1)
    
    grid.fit(X_train, Y_train)
    return grid.best_estimator_, std_in, std_out, X_test, Y_test

"""Function that will use the outcome of previous function, to plot the graph for any give output target variable for the regression model"""

def plot_target_var(trgt_names, y_true, y_pred):
    fig, axis = plt.subplots(nrows=1, ncols=len(trgt_names), figsize=(20, 7))
    axis = axis.ravel()
        
    for idx, name in enumerate(trgt_names):
        trues = y_true[:, idx]
        preds = y_pred[:, idx]
        
        ts = sorted(zip(trues, preds), key=lambda x: x[0])
        trues = list(zip(*ts))[0]
        preds = list(zip(*ts))[1]
        
        r_siz = range(len(preds))
        axis[idx].scatter(r_siz, trues, s=40, c='b', marker='o',
                          alpha=0.5, linewidths=1, edgecolors='b', label='Ground Truth')
        axis[idx].scatter(r_siz, preds, s=40, c='r', marker='x',
                          alpha=0.5, linewidths=1, edgecolors='r', label='Predictions')
        axis[idx].set_ylabel('{}'.format(name))
        axis[idx].set_xlabel('Ordered target variables')
        axis[idx].set_title(name)
        axis[idx].legend(loc=0)
    fig.tight_layout()

"""#### GridSearch with Cross Validation
<a id='grid'></a>

[Back to top](#top)
"""

mlp = MLPRegressor()
n_dim = 10
param_grid_mlp = {'hidden_layer_sizes': [(n_dim, n_dim, n_dim, n_dim, n_dim, n_dim, n_dim, n_dim, n_dim, n_dim, n_dim),
                                         (n_dim, n_dim, n_dim, n_dim, n_dim, n_dim, n_dim, n_dim, n_dim, n_dim),
                                         (n_dim, n_dim, n_dim, n_dim, n_dim, n_dim, n_dim, n_dim, n_dim),
                                         (n_dim, n_dim, n_dim, n_dim, n_dim, n_dim, n_dim),
                                         (n_dim, n_dim, n_dim, n_dim, n_dim, n_dim)],
                  'max_iter':[5000],
                  'activation': ['tanh', 'relu', 'logistic'],
                  'solver': ['lbfgs', 'sgd', 'adam'],
                  'learning_rate' : ['constant'],
                  'learning_rate_init': [0.001, 0.01, 0.1]}


best_estimator, std_in, std_out, X_test, Y_test = experiment_train_kfold(data=database_dipole_bw, 
                                                                         estimator=mlp,
                                                                         param_grid=param_grid_mlp,
                                                                         targets=['BW', 'f'],
                                                                         metric='neg_mean_squared_error', 
                                                                         normalize_target=False)

Y_pred = best_estimator.predict(X_test)
msq_err = mean_squared_error(Y_test, Y_pred)
print("Mean Squared Error: {}".format(msq_err))

best_estimator

"""#### Visualizing output of target Variables
<a id='target'></a>

[Back to top](#top)
"""

plot_target_var(trgt_names=['BW', 'f'], 
                y_true=Y_test.values, 
                y_pred=Y_pred)

"""### Exporting and loading the model with pickle

[Back to top](#top)
"""

import pickle

# poli-wocav2-v2.pickle requires
# sklearn '0.22.2.post1'

# poli-wocav2-v3.pickle requires
# sklearn '0.23.2'

# var_dict = {'regmodel': best_estimator,
#             'std_in': std_in,
#             'std_out': std_out}

# EXPORTING A NEW MODEL - MAKE SURE IT'S BETTER THAN THE PREVIOUS ONE
# with open('poli-wocav2-v3.pickle', 'wb') as pkc:
#     pickle.dump(var_dict, pkc)
    
# LOADING THE SAVED MODEL  
with open('poli-wocav2-v3.pickle', 'rb') as pck:
    var_dict = pickle.load(pck)

reg_model = var_dict['regmodel']
std_in = var_dict['std_in']
std_out = var_dict['std_out']

reg_model

"""### Definig our own MOO Problem with JMetalPy Inheritance
<a id='problem'></a>

[Back to top](#top)
"""

class DipoloSimplesMO2(FloatProblem):

    def __init__(self, number_of_variables: int = 3, number_of_objectives: int = 2, 
                       model_dict = None, lb = None, up = None):
        
        super(DipoloSimplesMO2, self).__init__()
        self.number_of_variables = number_of_variables
        self.number_of_objectives = number_of_objectives
        
        self.number_of_constraints = 0
        self.obj_directions = [self.MINIMIZE, self.MINIMIZE]
        self.obj_labels = ['BW', 'f']
            
        self.lower_bound = lb
        self.upper_bound = up
        
        self.reg_model = model_dict['regmodel']
        self.std_in = model_dict['std_in']
        self.name = 'DipoloSimplesMO2'
        
    def evaluate(self, solution: FloatSolution) -> FloatSolution:
        input_vars = np.array(solution.variables).reshape(1, -1)
        input_vars = self.std_in.transform(input_vars)
        objct_vals = self.reg_model.predict(input_vars)[0]
        objct_vals[0] = -1 * objct_vals[0]
        objct_vals[1] = np.abs(3.5 - objct_vals[1])
        solution.objectives = list(objct_vals)
        return solution
    
    def get_name(self):
        return self.name

"""### Running a Instance of our MOO Problem
<a id='run'> </a>

[Back to top](#top)

#### Retrieving the Parameter for our Problem
<a id='params'> </a>

[Back to top](#top)
"""

input_dimensions = database_dipole_bw.drop(labels=['BW', 'f'], axis='columns').values
outpt_dimensions = database_dipole_bw[['BW', 'f']].values

input_dimensions_std = std_in.transform(input_dimensions)

"""**Input Variables**"""

print('Minimal input dimensions: {}'.format(input_dimensions.min(axis=0)))
print('Maximum input dimensions: {}'.format(input_dimensions.max(axis=0)))

print('Minimal standardized input dimensions: {}'.format(input_dimensions_std.min(axis=0)))
print('Maximum standardized input dimensions: {}'.format(input_dimensions_std.max(axis=0)))

"""**Output Variables**"""

print('Minimal output dimensions: {}'.format(outpt_dimensions.min(axis=0)))
print('Maximum output dimensions: {}'.format(outpt_dimensions.max(axis=0)))

"""**Instantiating the Objective Function**"""

lower_bound = input_dimensions.min(axis=0)
upper_bound = input_dimensions.max(axis=0)
objective_function = DipoloSimplesMO2(model_dict=var_dict, lb=lower_bound, up=upper_bound)

"""#### Redefining the Algorithms
<a id='re_impl'> </a>

[Back to top](#top)
"""

import time
import logging

import numpy as np

from copy import copy
from jmetal.util.solution import get_non_dominated_solutions

LOGGER = logging.getLogger('jmetal')

"""**SMPSO**"""

class SMPSO(smpso.SMPSO):
    
    def __init__(self, **kwargs):
        super(SMPSO, self).__init__(**kwargs)
        self.front_history = dict()
                           
    def get_current_front(self):
        return copy(get_non_dominated_solutions(self.get_result()))
    
    def front_progress(self):
        sorted_keys = sorted(self.front_history.keys())
        for key in sorted_keys:
            front = self.front_history[key]
            yield key, front  
    
    def run(self):
        """ Execute the algorithm. """
        self.start_computing_time = time.time()
        self.solutions = self.create_initial_solutions()
        self.solutions = self.evaluate(self.solutions)

        LOGGER.debug('Initializing progress')
        self.init_progress()

        LOGGER.debug('Running main loop until termination criteria is met')
        iterations_index = 0
        while not self.stopping_condition_is_met():
            self.step()
            self.update_progress()
            self.front_history[iterations_index] = self.get_current_front()
            iterations_index += 1

        self.total_computing_time = time.time() - self.start_computing_time

"""**OMOPSO**"""

class OMOPSO(omopso.OMOPSO):
    
    def __init__(self, **kwargs):
        super(OMOPSO, self).__init__(**kwargs)
        self.front_history = dict()
                           
    def get_current_front(self):
        return copy(get_non_dominated_solutions(self.get_result()))
    
    def front_progress(self):
        sorted_keys = sorted(self.front_history.keys())
        for key in sorted_keys:
            front = self.front_history[key]
            yield key, front 
    
    def run(self):
        """ Execute the algorithm. """
        self.start_computing_time = time.time()
        self.solutions = self.create_initial_solutions()
        self.solutions = self.evaluate(self.solutions)

        LOGGER.debug('Initializing progress')
        self.init_progress()

        LOGGER.debug('Running main loop until termination criteria is met')
        iterations_index = 0
        while not self.stopping_condition_is_met():
            self.step()
            self.update_progress()
            self.front_history[iterations_index] = self.get_current_front()
            iterations_index += 1

        self.total_computing_time = time.time() - self.start_computing_time

"""**NSGA-II**"""

class NSGAII(nsgaii.NSGAII):
    
    def __init__(self, **kwargs):
        super(NSGAII, self).__init__(**kwargs)
        self.front_history = dict()
                           
    def get_current_front(self):
        return copy(get_non_dominated_solutions(self.get_result()))
    
    def front_progress(self):
        sorted_keys = sorted(self.front_history.keys())
        for key in sorted_keys:
            front = self.front_history[key]
            yield key, front   
    
    def run(self):
        """ Execute the algorithm. """
        self.start_computing_time = time.time()
        self.solutions = self.create_initial_solutions()
        self.solutions = self.evaluate(self.solutions)

        LOGGER.debug('Initializing progress')
        self.init_progress()

        LOGGER.debug('Running main loop until termination criteria is met')
        iterations_index = 0
        while not self.stopping_condition_is_met():
            self.step()
            self.update_progress()
            self.front_history[iterations_index] = self.get_current_front()
            iterations_index += 1

        self.total_computing_time = time.time() - self.start_computing_time

"""**SPEA2**"""

class SPEA2(spea2.SPEA2):
    
    def __init__(self, **kwargs):
        super(SPEA2, self).__init__(**kwargs)
        self.front_history = dict()
                           
    def get_current_front(self):
        return copy(get_non_dominated_solutions(self.get_result()))
    
    def front_progress(self):
        sorted_keys = sorted(self.front_history.keys())
        for key in sorted_keys:
            front = self.front_history[key]
            yield key, front
    
    def run(self):
        """ Execute the algorithm. """
        self.start_computing_time = time.time()
        self.solutions = self.create_initial_solutions()
        self.solutions = self.evaluate(self.solutions)

        LOGGER.debug('Initializing progress')
        self.init_progress()

        LOGGER.debug('Running main loop until termination criteria is met')
        iterations_index = 0
        while not self.stopping_condition_is_met():
            self.step()
            self.update_progress()
            self.front_history[iterations_index] = self.get_current_front()
            iterations_index += 1

        self.total_computing_time = time.time() - self.start_computing_time

"""**MOEA/D**"""

class MOEAD(moead.MOEAD):
    
    def __init__(self, **kwargs):
        super(MOEAD, self).__init__(**kwargs)
        self.front_history = dict()
                           
    def get_current_front(self):
        return copy(get_non_dominated_solutions(self.get_result()))
    
    def front_progress(self):
        sorted_keys = sorted(self.front_history.keys())
        for key in sorted_keys:
            front = self.front_history[key]
            yield key, front
    
    def run(self):
        """ Execute the algorithm. """
        self.start_computing_time = time.time()
        self.solutions = self.create_initial_solutions()
        self.solutions = self.evaluate(self.solutions)

        LOGGER.debug('Initializing progress')
        self.init_progress()

        LOGGER.debug('Running main loop until termination criteria is met')
        iterations_index = 0
        while not self.stopping_condition_is_met():
            self.step()
            self.update_progress()
            self.front_history[iterations_index] = self.get_current_front()
            iterations_index += 1

        self.total_computing_time = time.time() - self.start_computing_time

"""**IBEA**"""

class IBEA(ibea.IBEA):
    
    def __init__(self, **kwargs):
        super(IBEA, self).__init__(**kwargs)
        self.front_history = dict()
        
    def get_current_front(self):
        return copy(get_non_dominated_solutions(self.get_result()))
    
    def front_progress(self):
        sorted_keys = sorted(self.front_history.keys())
        for key in sorted_keys:
            front = self.front_history[key]
            yield key, front
            
    def run(self):
        """ Execute the algorithm. """
        self.start_computing_time = time.time()
        self.solutions = self.create_initial_solutions()
        self.solutions = self.evaluate(self.solutions)

        LOGGER.debug('Initializing progress')
        self.init_progress()

        LOGGER.debug('Running main loop until termination criteria is met')
        iterations_index = 0
        while not self.stopping_condition_is_met():
            self.step()
            self.update_progress()
            self.front_history[iterations_index] = self.get_current_front()
            iterations_index += 1

        self.total_computing_time = time.time() - self.start_computing_time

"""#### Running the Algorithms
<a id='re_run'> </a>

[Back to top](#top)
"""

swarm_size = 30
max_evaluations = 9000
mutation_probability = 1.0/objective_function.number_of_variables

"""**SMPSO**"""

smpso_enhanced = SMPSO(problem=objective_function,
                       swarm_size=swarm_size,
                       mutation=PolynomialMutation(probability=mutation_probability, distribution_index=20),
                       leaders=CrowdingDistanceArchive(100),
                       termination_criterion=StoppingByEvaluations(max_evaluations=max_evaluations))
smpso_enhanced.run()

print('Total computing time for SMPSO: {:.4f} s'.format(smpso_enhanced.total_computing_time))

"""**OMOPSO**"""

ompso_enhanced = OMOPSO(problem=objective_function,
                        swarm_size=swarm_size,
                        epsilon=0.0075,
                        uniform_mutation=UniformMutation(probability=mutation_probability, perturbation=0.5),
                        non_uniform_mutation=NonUniformMutation(mutation_probability, perturbation=0.5,
                                                                max_iterations=int(max_evaluations / swarm_size)),
                        leaders=CrowdingDistanceArchive(100),
                        termination_criterion=StoppingByEvaluations(max_evaluations=max_evaluations))
ompso_enhanced.run()

print('Total computing time for OMOPSO: {:.4f} s'.format(ompso_enhanced.total_computing_time))

"""**NSGA-II**"""

nsgaii_enhanced = NSGAII(problem=objective_function,
                         population_size=100,
                         offspring_population_size=100,
                         mutation=PolynomialMutation(probability=mutation_probability, distribution_index=20),
                         crossover=SBXCrossover(probability=1.0, distribution_index=20),
                         termination_criterion=StoppingByEvaluations(max_evaluations=max_evaluations))
nsgaii_enhanced.run()

print('Total computing time for NSGA-II: {:.4f} s'.format(nsgaii_enhanced.total_computing_time))

"""**SPEA2**"""

speaii_enhanced = SPEA2(problem=objective_function,
                        population_size=30,
                        offspring_population_size=30,
                        mutation=PolynomialMutation(probability=mutation_probability, distribution_index=20),
                        crossover=SBXCrossover(probability=1.0, distribution_index=20),
                        termination_criterion=StoppingByEvaluations(max_evaluations=max_evaluations))

speaii_enhanced.run()

print('Total computing time for SPEA2: {:.4f} s'.format(speaii_enhanced.total_computing_time))

"""**MOEA/D**"""

max_evaluations = 20000
mutation_probability = 1.0/objective_function.number_of_variables
moead_enhanced = MOEAD(problem=objective_function,
                        population_size=30,
                        crossover=DifferentialEvolutionCrossover(CR=1.0, F=0.5, K=0.5),
                        mutation=PolynomialMutation(probability=mutation_probability, distribution_index=20),
                        aggregative_function=Tschebycheff(dimension=objective_function.number_of_objectives),
                        neighbor_size=5,
                        neighbourhood_selection_probability=0.9,
                        max_number_of_replaced_solutions=2,
                        weight_files_path='resources/MOEAD_weights',
                        termination_criterion=StoppingByEvaluations(max_evaluations=max_evaluations))
moead_enhanced.run()

print('Total computing time for MOEA/D: {:.4f} s'.format(moead_enhanced.total_computing_time))

"""**IBEA**"""

mutation_probability = 1.0/objective_function.number_of_variables
ibea_enhanced = IBEA(problem=objective_function,
                     kappa=1.0,
                     population_size=100,
                     offspring_population_size=100,
                     mutation=PolynomialMutation(probability=mutation_probability, distribution_index=20),
                     crossover=SBXCrossover(probability=1.0, distribution_index=20),
                     termination_criterion=StoppingByEvaluations(max_evaluations))
ibea_enhanced.run()

print('Total computing time for IBEA: {:.4f} s'.format(ibea_enhanced.total_computing_time))

"""#### Front Analysis of Algorithms
<a id='enha_front'> </a>

[Back to top](#top)
"""

smpso_enhanced_front = get_non_dominated_solutions(smpso_enhanced.get_result())
ompso_enhanced_front = get_non_dominated_solutions(ompso_enhanced.get_result())
nsgaii_enhanced_front = get_non_dominated_solutions(nsgaii_enhanced.get_result())
speaii_enhanced_front = get_non_dominated_solutions(speaii_enhanced.get_result())
moead_enhanced_front = get_non_dominated_solutions(moead_enhanced.get_result())
ibea_enhanced_front = get_non_dominated_solutions(ibea_enhanced.get_result())


enhanced_fronts = [smpso_enhanced_front, ompso_enhanced_front, 
                   nsgaii_enhanced_front, speaii_enhanced_front,
                   moead_enhanced_front, ibea_enhanced_front]

labels = ['SMPSO-DipoloSimplesMO2', 'OMPSO-DipoloSimplesMO2',
          'NSGA2-DipoloSimplesMO2', 'SPEA2-DipoloSimplesMO2',
          'MOEAD-DipoloSimplesMO2', 'IBEA-DipoloSimplesMO2']

def get_raw_front(solutions, var_dict):
    l = get_non_dominated_solutions(solutions)
    return np.array(list(map(lambda p: p.objectives, l)))

fig, axis= plt.subplots(nrows=2, ncols=3, figsize=(20, 8))
axis = axis.ravel()
for f, l, ax in zip(enhanced_fronts, labels, axis):
    pairs = get_raw_front(f, var_dict)
    ax.scatter(pairs[:,0], pairs[:,1], s=50, c='w', marker='o',
               alpha=0.5, linewidths=1, edgecolors='b')
    ax.set_ylabel('Error |f - 3.5|')
    ax.set_xlabel('-1 * BW')
    ax.set_title(l)
    
fig.tight_layout()

"""**De-Normalized Fronts**"""

def get_front(solutions, var_dict):
    pareto_front = []
    std_in = var_dict['std_in']
    std_out = var_dict['std_out']
    best_estimator = var_dict['regmodel']
    for s in solutions:
        input_vars = np.array(s.variables).reshape(1, -1)
        input_vars = std_in.transform(input_vars)
        objct_vals = best_estimator.predict(input_vars)[0]
        pareto_front.append(objct_vals)
    return np.array(pareto_front)

fig, axis= plt.subplots(nrows=2, ncols=3, figsize=(20, 8))
axis = axis.ravel()
for f, l, ax in zip(enhanced_fronts, labels, axis):
    pairs = get_front(f, var_dict)
    ax.scatter(pairs[:,0], pairs[:,1], s=50, c='w', marker='o',
               alpha=0.5, linewidths=1, edgecolors='b')
    ax.set_ylabel('fcentral (GHz)')
    ax.set_xlabel('BW')
    ax.set_title(l)
fig.tight_layout()

"""#### Front Overllapin Comparison
<a id='over_visu'></a>

[Back to top](#top)
"""

colors = ['r', 'g', 'b', 'y', 'k', 'c']
fig, axis= plt.subplots(nrows=1, ncols=1, figsize=(15, 8))
for f, l, c in zip(enhanced_fronts, labels, colors):
    pairs = get_front(f, var_dict)
    axis.scatter(pairs[:,0], pairs[:,1], s=50, c='w', marker='o',
               alpha=0.5, linewidths=1, edgecolors=c, label=l)
    axis.set_ylabel('fcentral (GHz)')
    axis.set_xlabel('BW')
axis.legend(loc=0)

"""#### Exporting Reports"""

import os
PATH_TO_SAVE = os.path.join(os.getcwd(), 'dipolo-reports')

def get_csv_report(filename, front, var_dict, var_names=['E[mm]', 'L [mm]', 'm[mm]', 'BW', 'f']):
    stnd_inpt = var_dict['std_in']
    estimator = var_dict['regmodel']
    input_vars = np.array(list(map(lambda s: s.variables, front)))
    input_vars_std = stnd_inpt.transform(input_vars)
    otput_vars = best_estimator.predict(input_vars_std)
    final_data = np.hstack((input_vars, otput_vars))
    dataframe = pd.DataFrame(data=final_data, columns=var_names)
    dataframe.to_csv(filename, index=False)

for l, f in zip(labels, enhanced_fronts):
    path_to_save_csv = os.path.join(PATH_TO_SAVE, l) + '.csv'
    get_csv_report(filename=path_to_save_csv, front=f, var_dict=var_dict)

"""#### Front Convergence
<a id='front_conv'> </a>

[Back to top](#top)
"""

from tqdm import tqdm

from jmetal.core.quality_indicator import HyperVolume
from jmetal.core.quality_indicator import GenerationalDistance
from jmetal.core.quality_indicator import InvertedGenerationalDistance
from jmetal.core.quality_indicator import EpsilonIndicator

def get_front(solutions, var_dict):
    pareto_front = []
    std_in = var_dict['std_in']
    std_out = var_dict['std_out']
    best_estimator = var_dict['regmodel']
    for s in solutions:
        input_vars = np.array(s.variables).reshape(1, -1)
        input_vars = std_in.transform(input_vars)
        objct_vals = best_estimator.predict(input_vars)
        objct_vals = std_out.inverse_transform(objct_vals)[0]
        pareto_front.append(objct_vals)
    return np.array(pareto_front)

def get_raw_front(solutions, var_dict):
    l = get_non_dominated_solutions(solutions)
    return np.array(list(map(lambda p: p.objectives, l)))

def compute_hv(algorithm, upp_bound):
    hv_values = []
    for idx, front in algorithm.front_progress():
        f = get_raw_front(front, var_dict)
        hv_metric = HyperVolume(reference_point=upp_bound).compute(f)
        hv_values.append(hv_metric)
    return hv_values

def get_algorithm_instance(algo_name):
    algos = {'smpso': SMPSO(problem=objective_function,
                            swarm_size=swarm_size,
                            mutation=PolynomialMutation(probability=mutation_probability, distribution_index=20),
                            leaders=CrowdingDistanceArchive(100),
                            termination_criterion=StoppingByEvaluations(max_evaluations=max_evaluations)),
             
             'omopso': OMOPSO(problem=objective_function,
                              swarm_size=swarm_size,
                              epsilon=0.0075,
                              uniform_mutation=UniformMutation(probability=mutation_probability, perturbation=0.5),
                              non_uniform_mutation=NonUniformMutation(mutation_probability, perturbation=0.5,
                                                                      max_iterations=int(max_evaluations / swarm_size)),
                              leaders=CrowdingDistanceArchive(100),
                              termination_criterion=StoppingByEvaluations(max_evaluations=max_evaluations)),
             
             'nsgaii': NSGAII(problem=objective_function,
                              population_size=30,
                              offspring_population_size=30,
                              mutation=PolynomialMutation(probability=mutation_probability, distribution_index=20),
                              crossover=SBXCrossover(probability=1.0, distribution_index=20),
                              termination_criterion=StoppingByEvaluations(max_evaluations=max_evaluations)),
             
             'spea2': SPEA2(problem=objective_function,
                            population_size=30,
                            offspring_population_size=30,
                            mutation=PolynomialMutation(probability=mutation_probability, distribution_index=20),
                            crossover=SBXCrossover(probability=1.0, distribution_index=20),
                            termination_criterion=StoppingByEvaluations(max_evaluations=max_evaluations)),
             
             'moead': MOEAD(problem=objective_function,
                            population_size=30,
                            crossover=DifferentialEvolutionCrossover(CR=1.0, F=0.5, K=0.5),
                            mutation=PolynomialMutation(probability=mutation_probability, distribution_index=20),
                            aggregative_function=Tschebycheff(dimension=objective_function.number_of_objectives),
                            neighbor_size=5,
                            neighbourhood_selection_probability=0.9,
                            max_number_of_replaced_solutions=2,
                            weight_files_path='resources/MOEAD_weights',
                            termination_criterion=StoppingByEvaluations(max_evaluations=700)),
             
             'ibea': IBEA(problem=objective_function,
                     kappa=1.0,
                     population_size=30,
                     offspring_population_size=30,
                     mutation=PolynomialMutation(probability=mutation_probability, distribution_index=20),
                     crossover=SBXCrossover(probability=1.0, distribution_index=20),
                     termination_criterion=StoppingByEvaluations(max_evaluations))
            }
    return algos[algo_name]

def hv_convergenve(algorithm, upp_bound, n_simulations=30):
    runs = []
    for n in tqdm(range(n_simulations)):
        algo = get_algorithm_instance(algorithm)
        algo.run()
        hv = compute_hv(algo, upp_bound)
        runs.append(hv)
    return np.mean(runs, axis=0)

fig, axis= plt.subplots(nrows=2, ncols=3, figsize=(15, 7))
axis = axis.ravel()

reference_point = [0.0, 0.8]

smpso_hv_values = []
ompso_hv_values = []
nsgaii_hv_values = []
speaii_hv_values = []
moead_hv_values = []
ibea_hv_values = []

hv_vals = [smpso_hv_values, ompso_hv_values, 
           nsgaii_hv_values, speaii_hv_values,
           moead_hv_values, ibea_hv_values]

algos = ['smpso', 'omopso', 'nsgaii', 'spea2', 'moead', 'ibea']
for idx, (alg, ax) in enumerate(zip(algos, axis)):
    hv_vals[idx].extend(hv_convergenve(algorithm=alg, upp_bound=reference_point, n_simulations=30))
    ax.plot(range(len(hv_vals[idx])), hv_vals[idx], c='b', alpha=0.5)
    ax.set_ylabel('HV')
    ax.set_xlabel('Number of Iterations')
    ax.set_title('Convergence of HV Indicator - {}'.format(alg.upper()))
fig.tight_layout()
print('Referecent Point for HV: {}'.format(reference_point))

from sklearn.preprocessing import minmax_scale

fig, axis= plt.subplots(nrows=2, ncols=3, figsize=(15, 7))
axis = axis.ravel()

reference_point = [0.0, 0.8]
algos = ['smpso', 'omopso', 'nsgaii', 'spea2', 'moead', 'ibea']
for idx, (alg, ax) in enumerate(zip(algos, axis)):
    hv_values = minmax_scale(hv_vals[idx])
    ax.plot(range(len(hv_values)), hv_values, c='b', alpha=0.5)
    ax.set_ylabel('HV')
    ax.set_xlabel('Number of Iterations')
    ax.set_title('Convergence of HV Indicator - {}'.format(alg.upper()))
    
fig.tight_layout()
fig.savefig('dipolo-figs/One-to-One Front Convergence Comparison - HV Indicator.jpg')
print('Referecent Point for HV: {}'.format(reference_point))

"""#### Normalized Front Convergence Comparison
<a id='normal_front_conv'> </a>

[Back to top](#top)
"""

from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import minmax_scale

colors = ['r', 'g', 'b', 'y', 'k', 'c']
algos = ['smpso', 'omopso', 'nsgaii', 'spea2', 'moead', 'ibea']

fig, axis = plt.subplots(nrows=1, ncols=1, figsize=(15, 8))

for alg, c, hv in zip(algos, colors, hv_vals):
    hv_values = minmax_scale(hv)
    axis.plot(range(len(hv_values)), hv_values, c=c, alpha=0.5, label=alg.upper())
    axis.set_ylabel('HV Indicator')
    axis.set_xlabel('Number of Iterations')
    
axis.set_title('Normalized Front Convergence Comparison - HV Indicator')
axis.legend(loc=0)
fig.savefig('dipolo-figs/Normalized Front Convergence Comparison - HV Indicator.jpg')

"""#### Spacing and Spreading
<a id='spacing_spread'> </a>

[Back to top](#top)
"""

np.set_printoptions(suppress=True)

def normalized_front(front):
    _, k = front.shape
    normalized_front = copy(front)
    for k_th in range(k):
        normalized_front[:, k_th] = (normalized_front[:, k_th] - \
                                     np.min(normalized_front[:, k_th]))/\
                                    (np.max(normalized_front[:, k_th]) - \
                                     np.min(normalized_front[:, k_th]))
    return normalized_front


class HV(object):
    
    @staticmethod
    def compute(front, upp_bound):
        return HyperVolume(reference_point=upp_bound).compute(front)
    

class Spacing(object):
    
    @staticmethod
    def compute(front):
        n_front = normalized_front(front)
        distances = []
        for p in n_front:
            min_dist = map(lambda x: np.linalg.norm(p - x), n_front)
            try:
                min_dist = sorted(min_dist)[1]
            except:
                min_dist = 0.0
            distances.append(min_dist)
        distances = np.array(distances)
        return np.sqrt((1/len(n_front)) * np.sum(np.power((distances - np.mean(distances)), 2)))
    
    
class Spread(object):
    
    @staticmethod
    def compute(front):
        n_front = normalized_front(front)
        distances = []
        for p in n_front:
            min_dist = map(lambda x: np.linalg.norm(p - x), n_front)
            try:
                min_dist = sorted(min_dist)[1]
            except:
                min_dist = 0.0
            distances.append(min_dist)
        distances = np.array(distances)
        d_bar = np.mean(distances)
        sum_d = np.sum(np.abs(distances - np.mean(distances)))
        
        _, k = n_front.shape
        d_ek = []
        for k_th in range(k):
            d_ek.append(max(n_front, key=lambda s: s[k_th]))
        
        d_ek_distances = []
        for d in d_ek:
            min_dist = map(lambda x: np.linalg.norm(d - x), n_front)
            try:
                min_dist = sorted(min_dist)[1]
            except:
                min_dist = 0.0
            d_ek_distances.append(min_dist)
            
        d_ek = sum(d_ek_distances)
        return (d_ek + sum_d)/(d_ek + len(n_front) * d_bar)

def spacing_convergenve(algorithm, n_simulations=30):
    runs = []
    for n in tqdm(range(n_simulations)):
        algo = get_algorithm_instance(algorithm)
        algo.run()
        front = get_raw_front(get_non_dominated_solutions(algo.get_result()), var_dict)
        spacing = Spacing.compute(front)
        runs.append(spacing)
    return runs


def spread_convergenve(algorithm, n_simulations=30):
    runs = []
    for n in tqdm(range(n_simulations)):
        algo = get_algorithm_instance(algorithm)
        algo.run()
        front = get_raw_front(get_non_dominated_solutions(algo.get_result()), var_dict)
        spread = Spread.compute(front)
        runs.append(spread)
    return runs


def hypervolume_convergenve(algorithm, n_simulations=30):
    runs = []
    for n in tqdm(range(n_simulations)):
        algo = get_algorithm_instance(algorithm)
        algo.run()
        front = get_raw_front(get_non_dominated_solutions(algo.get_result()), var_dict)
        hv = HV.compute(front, reference_point)
        runs.append(hv)
    return runs


def n_solutions_convergenge(algorithm, n_simulations=30):
    runs = []
    for n in tqdm(range(n_simulations)):
        algo = get_algorithm_instance(algorithm)
        algo.run()
        runs.append(len(algo.get_result()))
    return runs

algos = ['smpso', 'omopso', 'nsgaii', 'spea2', 'moead', 'ibea']
reference_point = [0.0, 0.8]

smpso_spacing_values = []
ompso_spacing_values = []
nsgaii_spacing_values = []
speaii_spacing_values = []
moead_spacing_values = []
ibea_spacing_values = []

spacing_vals = [smpso_spacing_values, ompso_spacing_values, 
                nsgaii_spacing_values, speaii_spacing_values,
                moead_spacing_values, ibea_spacing_values]

smpso_spread_values = []
ompso_spread_values = []
nsgaii_spread_values = []
speaii_spread_values = []
moead_spread_values = []
ibea_spread_values = []

spread_vals = [smpso_spread_values, ompso_spread_values, 
               nsgaii_spread_values, speaii_spread_values,
               moead_spread_values, ibea_spread_values]


smpso_hypervolume_values = []
ompso_hypervolume_values = []
nsgaii_hypervolume_values = []
speaii_hypervolume_values = []
moead_hypervolume_values = []
ibea_hypervolume_values = []

hypervolume_vals = [smpso_hypervolume_values, ompso_hypervolume_values, 
                    nsgaii_hypervolume_values, speaii_hypervolume_values,
                    moead_hypervolume_values, ibea_hypervolume_values]

smpso_nsolutions_values = []
ompso_nsolutions_values = []
nsgaii_nsolutions_values = []
speaii_nsolutions_values = []
moead_nsolutions_values = []
ibea_nsolutions_values = []

nsolutions_vals = [smpso_nsolutions_values, ompso_nsolutions_values, 
                   nsgaii_nsolutions_values, speaii_nsolutions_values,
                   moead_nsolutions_values, ibea_nsolutions_values]

for idx, alg in enumerate(algos):
    spacing_vals[idx].extend(spacing_convergenve(algorithm=alg, n_simulations=10))
    spread_vals[idx].extend(spread_convergenve(algorithm=alg, n_simulations=10))
    hypervolume_vals[idx].extend(hypervolume_convergenve(algorithm=alg, n_simulations=10))
#     nsolutions_vals[idx].extend(n_solutions_convergenge(algorithm=alg, n_simulations=10))

algos = ['smpso', 'omopso', 'nsgaii', 'spea2', 'moead', 'ibea']

metrics = {
    'HV': hypervolume_vals,
    'S': spacing_vals,
    'Sp': spread_vals,
#     'Sols': nsolutions_vals
}

def truncate(f, n):
    '''Truncates/pads a float f to n decimal places without rounding'''
    s = '{}'.format(f)
    if 'e' in s or 'E' in s:
        return '{0:.{1}f}'.format(f, n)
    i, p, d = s.partition('.')
    return '.'.join([i, (d+'0'*n)[:n]])
    

for algo in algos:
    print('\t\t' + algo, end='')
    
print('\n')
for metric in metrics:
    print(metric, end='')
    for vals in metrics[metric]:
        v = truncate(np.mean(vals), 3) if not truncate(np.mean(vals), 3) == 'nan.000' else '0.000'
        s = truncate(np.std(vals), 3) if not truncate(np.std(vals), 3) == 'nan.000' else '0.000'
        print('\t{}(+-{})'.format(v, s), end='')
    print('\n')

