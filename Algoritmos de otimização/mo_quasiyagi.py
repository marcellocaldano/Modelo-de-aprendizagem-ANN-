# -*- coding: utf-8 -*-
"""mo-QuasiYagi3OBJ-V2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1W1EtSP3B0j1RiW9A1_o5FUJ9KYI3rc6G

# Multi-Objetctive Experiments | Base Quasi-Yagi

In this Jupyter Notebook we will show you how to define from scrath a MOO (Multi Objective Optimization Problem) that is modeled from a previous trained regression model.

We will be using the JMetalPy library for the MO Optimization and the standard SKlearn Library and PyData Libraries for Machine Learning

### Tables of Contents
<a id='top'></a>

1. [Loading Essential Libraries](#loading)
2. [Loading JMetal Libraries](#jmetal)
3. [Loading Database for Regression](#data)
    1. [Data Visualization](#visu)
4. [Exporting and loading the model with pickle](#pickle)
5. [Definig our own MOO Problem with JMetalPy Inheritance](#problem)
6. [Running a Instance of our MOO Problem](#run)
   1. [Redefining the Algorithms](#re_impl)<br>
   2. [Running the Algorithms](#re_run)<br>
   3. [Running Times](#times)<br>
   4. [Front Analysis of Algorithms](#enha_front)<br>
   5. [Front Overllapin Comparison](#over_visu)<br>
   6. [Exporting Reports](#reports)<br>
   7. [Front Convergence](#front_conv)<br>
   8. [Normalized Front Convergence Comparison](#normal_front_conv)<br>

### Loading Essential Libraries
<a id='loading'></a>

[Back to top](#top)
"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline

import os

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

from sklearn.svm import SVR
from sklearn.neural_network import MLPRegressor

from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import KFold
from sklearn.model_selection import GridSearchCV

from sklearn.metrics import mean_absolute_error
from sklearn.metrics import mean_squared_error

from sklearn.multioutput import MultiOutputRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import MinMaxScaler

from tensorflow import keras

from sklearn.pipeline import Pipeline
from IPython.display import display

"""### Loading JMETAL Library
<a id='jmetal'></a>

[Back to top](#top)
"""

import pickle

from jmetal.core.problem import FloatProblem
from jmetal.core.solution import FloatSolution

from jmetal.algorithm.multiobjective import smpso
from jmetal.algorithm.multiobjective import nsgaii
from jmetal.algorithm.multiobjective import omopso
from jmetal.algorithm.multiobjective import spea2
from jmetal.algorithm.multiobjective import moead
from jmetal.algorithm.multiobjective import ibea


from jmetal.operator import SBXCrossover
from jmetal.operator import PolynomialMutation
from jmetal.operator import DifferentialEvolutionCrossover

from jmetal.util.aggregative_function import Tschebycheff

from jmetal.util.termination_criterion import StoppingByEvaluations
from jmetal.util.termination_criterion import QualityIndicator

from jmetal.util.archive import CrowdingDistanceArchive

from jmetal.operator import UniformMutation
from jmetal.operator.mutation import NonUniformMutation

"""### Loading Database for Regression
<a id='data'></a>

[Back to top](#top)
"""

from IPython.display import display

database_quasi_yagi = pd.read_excel('data/quasi-yagi-output_S11_final.xlsx')
display(database_quasi_yagi.head())

print(len(database_quasi_yagi))

plt.figure(figsize=(10, 8))
corr = database_quasi_yagi.corr()
sns.heatmap(corr, annot=True, cbar=False, cmap='Reds')

"""#### Data Visualization
<a id='visu'></a>
[Back to top](#top)
"""

fig, axis = plt.subplots(nrows=1, ncols=3, figsize=(20, 4))
axis = axis.ravel()

sns.distplot(database_quasi_yagi['RF1'], ax=axis[0])
axis[0].set_title('Distribuicao de RF1 na base Quasi-Yagi')

sns.distplot(database_quasi_yagi['RF2'], ax=axis[1])
axis[1].set_title('Distribuicao de RF2 na base Quasi-Yagi')

sns.distplot(database_quasi_yagi['RF3'], ax=axis[2])
axis[2].set_title('Distribuicao de RF3 na base Quasi-Yagi')

fig.tight_layout()

"""### Loading the model with pickle
<a id='pickle'></a>

[Back to top](#top)
"""

import pickle
from tensorflow import keras

model_loaded = keras.models.load_model('poli-woca-quasi-yagi-v3.model')  
    
with open('poli-woca-quasi-yagi-v3.pickle', 'rb') as pck:
    var_dict = pickle.load(pck)

std_in = var_dict['std_in']
std_out = var_dict['std_out']

print('Model loaded: {}'.format(model_loaded))
print('Input StandarScaler loaded: {}'.format(std_in))
print('Output StandarScaler loaded: {}'.format(std_out))

"""### Definig our own MOO Problem with JMetalPy Inheritance
<a id='problem'></a>

[Back to top](#top)
"""

class QuasiYagiMO3OBJ(FloatProblem):

    def __init__(self, number_of_variables: int = 6, number_of_objectives: int = 3, 
                       keras_model = None, model_dict = None, lb = None, up = None):
        
        super(QuasiYagiMO3OBJ, self).__init__()
        self.number_of_variables = number_of_variables
        self.number_of_objectives = number_of_objectives
        
        self.number_of_constraints = 0
        self.obj_directions = [self.MINIMIZE, self.MINIMIZE, self.MINIMIZE]
        self.obj_labels = ['RF1', 'RF2', 'RF3']
            
        self.lower_bound = lb
        self.upper_bound = up
        
        self.reg_model = keras_model
        self.std_in = model_dict['std_in']
        self.std_out = model_dict['std_out']

        self.name = 'QuasiYagiMO3OBJ'
        
    def evaluate(self, solution: FloatSolution) -> FloatSolution:
        input_vars = np.array(solution.variables).reshape(1, -1)
        input_vars = self.std_in.transform(input_vars)
        objct_vals = self.reg_model.predict(input_vars)[0]
        solution.objectives = list(objct_vals)
        return solution
    
    def get_name(self):
        return self.name

"""### Running a Instance of our MOO Problem
<a id='run'> </a>

[Back to top](#top)

#### Retrieving the Parameter for our Problem
<a id='params'> </a>

[Back to top](#top)
"""

input_dimensions = database_quasi_yagi.drop(labels=['RF1', 'RF2', 'RF3'], axis='columns').values
outpt_dimensions = database_quasi_yagi[['RF1', 'RF2', 'RF3']].values

input_dimensions_std = std_in.transform(input_dimensions)

"""**Input Variables**"""

print('Minimal input dimensions: {}'.format(input_dimensions.min(axis=0)))
print('Maximum input dimensions: {}'.format(input_dimensions.max(axis=0)))

print('Minimal standardized input dimensions: {}'.format(input_dimensions_std.min(axis=0)))
print('Maximum standardized input dimensions: {}'.format(input_dimensions_std.max(axis=0)))

"""**Output Variables**"""

print('Minimal output dimensions: {}'.format(outpt_dimensions.min(axis=0)))
print('Maximum output dimensions: {}'.format(outpt_dimensions.max(axis=0)))

"""**Instantiating the Objective Function**"""

lower_bound = input_dimensions.min(axis=0)
upper_bound = input_dimensions.max(axis=0)
objective_function = QuasiYagiMO3OBJ(keras_model=model_loaded, model_dict=var_dict,
                                     lb=lower_bound, up=upper_bound)

"""#### Redefining the Algorithms
<a id='re_impl'> </a>

[Back to top](#top)
"""

import time
import logging

import numpy as np

from copy import copy
from jmetal.util.solution import get_non_dominated_solutions

LOGGER = logging.getLogger('jmetal')

"""**SMPSO**"""

class SMPSO(smpso.SMPSO):
    
    def __init__(self, **kwargs):
        super(SMPSO, self).__init__(**kwargs)
        self.front_history = dict()
                           
    def get_current_front(self):
        return copy(get_non_dominated_solutions(self.get_result()))
    
    def front_progress(self):
        sorted_keys = sorted(self.front_history.keys())
        for key in sorted_keys:
            front = self.front_history[key]
            yield key, front  
    
    def run(self):
        """ Execute the algorithm. """
        self.start_computing_time = time.time()
        self.solutions = self.create_initial_solutions()
        self.solutions = self.evaluate(self.solutions)

        LOGGER.debug('Initializing progress')
        self.init_progress()

        LOGGER.debug('Running main loop until termination criteria is met')
        iterations_index = 0
        while not self.stopping_condition_is_met():
            self.step()
            self.update_progress()
            self.front_history[iterations_index] = self.get_current_front()
            iterations_index += 1

        self.total_computing_time = time.time() - self.start_computing_time

"""**OMOPSO**"""

class OMOPSO(omopso.OMOPSO):
    
    def __init__(self, **kwargs):
        super(OMOPSO, self).__init__(**kwargs)
        self.front_history = dict()
                           
    def get_current_front(self):
        return copy(get_non_dominated_solutions(self.get_result()))
    
    def front_progress(self):
        sorted_keys = sorted(self.front_history.keys())
        for key in sorted_keys:
            front = self.front_history[key]
            yield key, front 
    
    def run(self):
        """ Execute the algorithm. """
        self.start_computing_time = time.time()
        self.solutions = self.create_initial_solutions()
        self.solutions = self.evaluate(self.solutions)

        LOGGER.debug('Initializing progress')
        self.init_progress()

        LOGGER.debug('Running main loop until termination criteria is met')
        iterations_index = 0
        while not self.stopping_condition_is_met():
            self.step()
            self.update_progress()
            self.front_history[iterations_index] = self.get_current_front()
            iterations_index += 1

        self.total_computing_time = time.time() - self.start_computing_time

"""**NSGA-II**"""

class NSGAII(nsgaii.NSGAII):
    
    def __init__(self, **kwargs):
        super(NSGAII, self).__init__(**kwargs)
        self.front_history = dict()
                           
    def get_current_front(self):
        return copy(get_non_dominated_solutions(self.get_result()))
    
    def front_progress(self):
        sorted_keys = sorted(self.front_history.keys())
        for key in sorted_keys:
            front = self.front_history[key]
            yield key, front   
    
    def run(self):
        """ Execute the algorithm. """
        self.start_computing_time = time.time()
        self.solutions = self.create_initial_solutions()
        self.solutions = self.evaluate(self.solutions)

        LOGGER.debug('Initializing progress')
        self.init_progress()

        LOGGER.debug('Running main loop until termination criteria is met')
        iterations_index = 0
        while not self.stopping_condition_is_met():
            self.step()
            self.update_progress()
            self.front_history[iterations_index] = self.get_current_front()
            iterations_index += 1

        self.total_computing_time = time.time() - self.start_computing_time

"""**SPEA2**"""

class SPEA2(spea2.SPEA2):
    
    def __init__(self, **kwargs):
        super(SPEA2, self).__init__(**kwargs)
        self.front_history = dict()
                           
    def get_current_front(self):
        return copy(get_non_dominated_solutions(self.get_result()))
    
    def front_progress(self):
        sorted_keys = sorted(self.front_history.keys())
        for key in sorted_keys:
            front = self.front_history[key]
            yield key, front
    
    def run(self):
        """ Execute the algorithm. """
        self.start_computing_time = time.time()
        self.solutions = self.create_initial_solutions()
        self.solutions = self.evaluate(self.solutions)

        LOGGER.debug('Initializing progress')
        self.init_progress()

        LOGGER.debug('Running main loop until termination criteria is met')
        iterations_index = 0
        while not self.stopping_condition_is_met():
            self.step()
            self.update_progress()
            self.front_history[iterations_index] = self.get_current_front()
            iterations_index += 1

        self.total_computing_time = time.time() - self.start_computing_time

"""**MOEA/D**"""

class MOEAD(moead.MOEAD):
    
    def __init__(self, **kwargs):
        super(MOEAD, self).__init__(**kwargs)
        self.front_history = dict()
                           
    def get_current_front(self):
        return copy(get_non_dominated_solutions(self.get_result()))
    
    def front_progress(self):
        sorted_keys = sorted(self.front_history.keys())
        for key in sorted_keys:
            front = self.front_history[key]
            yield key, front
    
    def run(self):
        """ Execute the algorithm. """
        self.start_computing_time = time.time()
        self.solutions = self.create_initial_solutions()
        self.solutions = self.evaluate(self.solutions)

        LOGGER.debug('Initializing progress')
        self.init_progress()

        LOGGER.debug('Running main loop until termination criteria is met')
        iterations_index = 0
        while not self.stopping_condition_is_met():
            self.step()
            self.update_progress()
            self.front_history[iterations_index] = self.get_current_front()
            iterations_index += 1

        self.total_computing_time = time.time() - self.start_computing_time

"""**IBEA**"""

class IBEA(ibea.IBEA):
    
    def __init__(self, **kwargs):
        super(IBEA, self).__init__(**kwargs)
        self.front_history = dict()
        
    def get_current_front(self):
        return copy(get_non_dominated_solutions(self.get_result()))
    
    def front_progress(self):
        sorted_keys = sorted(self.front_history.keys())
        for key in sorted_keys:
            front = self.front_history[key]
            yield key, front
            
    def run(self):
        """ Execute the algorithm. """
        self.start_computing_time = time.time()
        self.solutions = self.create_initial_solutions()
        self.solutions = self.evaluate(self.solutions)

        LOGGER.debug('Initializing progress')
        self.init_progress()

        LOGGER.debug('Running main loop until termination criteria is met')
        iterations_index = 0
        while not self.stopping_condition_is_met():
            self.step()
            self.update_progress()
            self.front_history[iterations_index] = self.get_current_front()
            iterations_index += 1

        self.total_computing_time = time.time() - self.start_computing_time

"""#### Running the Algorithms
<a id='re_run'> </a>

[Back to top](#top)
"""

swarm_size = 30
max_evaluations = 9000
mutation_probability = 1.0/objective_function.number_of_variables

"""**SMPSO**"""

smpso_enhanced = SMPSO(problem=objective_function,
                       swarm_size=swarm_size,
                       mutation=PolynomialMutation(probability=mutation_probability, distribution_index=20),
                       leaders=CrowdingDistanceArchive(100),
                       termination_criterion=StoppingByEvaluations(max_evaluations=max_evaluations))
smpso_enhanced.run()

"""**OMOPSO**"""

ompso_enhanced = OMOPSO(problem=objective_function,
                        swarm_size=swarm_size,
                        epsilon=0.0075,
                        uniform_mutation=UniformMutation(probability=mutation_probability, perturbation=0.5),
                        non_uniform_mutation=NonUniformMutation(mutation_probability, perturbation=0.5,
                                                                max_iterations=int(max_evaluations / swarm_size)),
                        leaders=CrowdingDistanceArchive(100),
                        termination_criterion=StoppingByEvaluations(max_evaluations=max_evaluations))
ompso_enhanced.run()

"""**NSGA-II**"""

nsgaii_enhanced = NSGAII(problem=objective_function,
                         population_size=100,
                         offspring_population_size=100,
                         mutation=PolynomialMutation(probability=mutation_probability, distribution_index=20),
                         crossover=SBXCrossover(probability=1.0, distribution_index=20),
                         termination_criterion=StoppingByEvaluations(max_evaluations=max_evaluations))
nsgaii_enhanced.run()

"""**SPEA2**"""

speaii_enhanced = SPEA2(problem=objective_function,
                        population_size=30,
                        offspring_population_size=30,
                        mutation=PolynomialMutation(probability=mutation_probability, distribution_index=20),
                        crossover=SBXCrossover(probability=1.0, distribution_index=20),
                        termination_criterion=StoppingByEvaluations(max_evaluations=max_evaluations))

speaii_enhanced.run()

"""**MOEA/D**"""

max_evaluations = 20000
mutation_probability = 1.0/objective_function.number_of_variables
moead_enhanced = MOEAD(problem=objective_function,
                        population_size=100,
                        crossover=DifferentialEvolutionCrossover(CR=1.0, F=0.5, K=0.5),
                        mutation=PolynomialMutation(probability=mutation_probability, distribution_index=20),
                        aggregative_function=Tschebycheff(dimension=objective_function.number_of_objectives),
                        neighbor_size=5,
                        neighbourhood_selection_probability=0.9,
                        max_number_of_replaced_solutions=2,
                        weight_files_path=os.getcwd(),
                        termination_criterion=StoppingByEvaluations(max_evaluations=max_evaluations))
moead_enhanced.run()

"""**IBEA**"""

mutation_probability = 1.0/objective_function.number_of_variables
ibea_enhanced = IBEA(problem=objective_function,
                     kappa=1.0,
                     population_size=100,
                     offspring_population_size=100,
                     mutation=PolynomialMutation(probability=mutation_probability, distribution_index=20),
                     crossover=SBXCrossover(probability=1.0, distribution_index=20),
                     termination_criterion=StoppingByEvaluations(max_evaluations))
ibea_enhanced.run()

"""#### Running Times
<a id='times'> </a>

[Back to top](#top)
"""

print('Total computing time for SMPSO: {:.4f} s'.format(smpso_enhanced.total_computing_time))
print('Total computing time for OMOPSO: {:.4f} s'.format(ompso_enhanced.total_computing_time))
print('Total computing time for NSGA-II: {:.4f} s'.format(nsgaii_enhanced.total_computing_time))
print('Total computing time for SPEA2: {:.4f} s'.format(speaii_enhanced.total_computing_time))
print('Total computing time for MOEA/D: {:.4f} s'.format(moead_enhanced.total_computing_time))
print('Total computing time for IBEA: {:.4f} s'.format(ibea_enhanced.total_computing_time))

"""#### Front Analysis of Algorithms
<a id='enha_front'> </a>

[Back to top](#top)
"""

smpso_enhanced_front = get_non_dominated_solutions(smpso_enhanced.get_result())
ompso_enhanced_front = get_non_dominated_solutions(ompso_enhanced.get_result())
nsgaii_enhanced_front = get_non_dominated_solutions(nsgaii_enhanced.get_result())
speaii_enhanced_front = get_non_dominated_solutions(speaii_enhanced.get_result())
moead_enhanced_front = get_non_dominated_solutions(moead_enhanced.get_result())
ibea_enhanced_front = get_non_dominated_solutions(ibea_enhanced.get_result())


enhanced_fronts = [smpso_enhanced_front, ompso_enhanced_front, 
                   nsgaii_enhanced_front, speaii_enhanced_front,
                   moead_enhanced_front, ibea_enhanced_front]

labels = ['SMPSO-QuasiYagiMO3OBJ', 'OMPSO-QuasiYagiMO3OBJ',
          'NSGA2-QuasiYagiMO3OBJ', 'SPEA2-QuasiYagiMO3OBJ',
          'MOEAD-QuasiYagiMO3OBJ', 'IBEA-QuasiYagiMO3OBJ']

from mpl_toolkits.mplot3d import Axes3D

def get_raw_front(solutions, var_dict):
    l = get_non_dominated_solutions(solutions)
    return np.array(list(map(lambda p: p.objectives, l)))

fig, axis= plt.subplots(nrows=2, ncols=3, figsize=(18, 10), subplot_kw={'projection': '3d'})
axis = axis.ravel()
for f, l, ax in zip(enhanced_fronts, labels, axis):
    tuples = get_raw_front(f, var_dict)
    ax.scatter(tuples[:,0], tuples[:,1], tuples[:,2], c='w', marker='o',
               alpha=0.5, linewidths=1, edgecolors='b')
    ax.set_xlabel('$RF1$', fontsize=13)
    ax.set_ylabel('$RF2$', fontsize=13)
    ax.set_zlabel('$RF3$', fontsize=13)
    ax.set_title(l)
    
fig.tight_layout()
fig.savefig('quasi-figs/Pareto Front - Original.jpg')
fig.savefig('quasi-figs/Pareto Front - Transformed.jpg')

"""#### Front Overllaping Comparison
<a id='over_visu'></a>

[Back to top](#top)
"""

colors = ['r', 'g', 'b', 'y', 'k', 'c']
fig, axis= plt.subplots(nrows=1, ncols=1, figsize=(15, 12), subplot_kw={'projection': '3d'})
for f, l, c in zip(enhanced_fronts, labels, colors):
    tuples = get_raw_front(f, var_dict)
    axis.scatter(tuples[:,0], tuples[:,1], tuples[:,2], c='w', marker='o',
               alpha=0.5, linewidths=1, edgecolors=c, label=l)
    axis.set_ylabel('RF1')
    axis.set_xlabel('RF2')
    axis.set_zlabel('RF3')
axis.legend(loc=0)
fig.savefig('quasi-figs/Pareto Front - Comparison.jpg')

"""#### Exporting Reports
<a id='reports'> </a>

[Back to top](#top)
"""

import os
PATH_TO_SAVE = os.path.join(os.getcwd(), 'quasi-reports')

def get_csv_report(filename, front, keras_model, std_in, var_names):
    input_vars = np.array(list(map(lambda s: s.variables, front)))
    input_vars_std = std_in.transform(input_vars)
    otput_vars = keras_model.predict(input_vars_std)
    final_data = np.hstack((input_vars, otput_vars))
    dataframe = pd.DataFrame(data=final_data, columns=var_names)
    dataframe.to_csv(filename, index=False)

var_names = database_quasi_yagi.columns
for l, f in zip(labels, enhanced_fronts):
    path_to_save_csv = os.path.join(PATH_TO_SAVE, l) + '.csv'
    get_csv_report(filename=path_to_save_csv, front=f, keras_model=model_loaded, std_in=std_in, var_names=var_names)

"""#### Front Convergence
<a id='front_conv'> </a>

[Back to top](#top)
"""

from tqdm import tqdm

from jmetal.core.quality_indicator import HyperVolume
from jmetal.core.quality_indicator import GenerationalDistance
from jmetal.core.quality_indicator import InvertedGenerationalDistance
from jmetal.core.quality_indicator import EpsilonIndicator

swarm_size = 30
max_evaluations = 20000
mutation_probability = 1.0/objective_function.number_of_variables

def get_front(solutions, var_dict):
    pareto_front = []
    std_in = var_dict['std_in']
    std_out = var_dict['std_out']
    best_estimator = var_dict['regmodel']
    for s in solutions:
        input_vars = np.array(s.variables).reshape(1, -1)
        input_vars = std_in.transform(input_vars)
        objct_vals = best_estimator.predict(input_vars)
        objct_vals = std_out.inverse_transform(objct_vals)[0]
        pareto_front.append(objct_vals)
    return np.array(pareto_front)

def get_raw_front(solutions, var_dict):
    l = get_non_dominated_solutions(solutions)
    return np.array(list(map(lambda p: p.objectives, l)))

def compute_hv(algorithm, upp_bound):
    hv_values = []
    for idx, front in algorithm.front_progress():
        f = get_raw_front(front, var_dict)
        hv_metric = HyperVolume(reference_point=upp_bound).compute(f)
        hv_values.append(hv_metric)
    return hv_values

def get_algorithm_instance(algo_name):
    algos = {'smpso': SMPSO(problem=objective_function,
                            swarm_size=swarm_size,
                            mutation=PolynomialMutation(probability=mutation_probability, distribution_index=20),
                            leaders=CrowdingDistanceArchive(100),
                            termination_criterion=StoppingByEvaluations(max_evaluations=max_evaluations)),
             
             'omopso': OMOPSO(problem=objective_function,
                              swarm_size=swarm_size,
                              epsilon=0.0075,
                              uniform_mutation=UniformMutation(probability=mutation_probability, perturbation=0.5),
                              non_uniform_mutation=NonUniformMutation(mutation_probability, perturbation=0.5,
                                                                      max_iterations=int(max_evaluations / swarm_size)),
                              leaders=CrowdingDistanceArchive(100),
                              termination_criterion=StoppingByEvaluations(max_evaluations=max_evaluations)),
             
             'nsgaii': NSGAII(problem=objective_function,
                              population_size=30,
                              offspring_population_size=30,
                              mutation=PolynomialMutation(probability=mutation_probability, distribution_index=20),
                              crossover=SBXCrossover(probability=1.0, distribution_index=20),
                              termination_criterion=StoppingByEvaluations(max_evaluations=max_evaluations)),
             
             'spea2': SPEA2(problem=objective_function,
                            population_size=30,
                            offspring_population_size=30,
                            mutation=PolynomialMutation(probability=mutation_probability, distribution_index=20),
                            crossover=SBXCrossover(probability=1.0, distribution_index=20),
                            termination_criterion=StoppingByEvaluations(max_evaluations=max_evaluations)),
             
             'moead': MOEAD(problem=objective_function,
                            population_size=100,
                            crossover=DifferentialEvolutionCrossover(CR=1.0, F=0.5, K=0.5),
                            mutation=PolynomialMutation(probability=mutation_probability, distribution_index=20),
                            aggregative_function=Tschebycheff(dimension=objective_function.number_of_objectives),
                            neighbor_size=5,
                            neighbourhood_selection_probability=0.9,
                            max_number_of_replaced_solutions=2,
                            weight_files_path=os.getcwd(),
                            termination_criterion=StoppingByEvaluations(max_evaluations=700)),
             
             'ibea': IBEA(problem=objective_function,
                     kappa=1.0,
                     population_size=30,
                     offspring_population_size=30,
                     mutation=PolynomialMutation(probability=mutation_probability, distribution_index=20),
                     crossover=SBXCrossover(probability=1.0, distribution_index=20),
                     termination_criterion=StoppingByEvaluations(max_evaluations))
            }
    return algos[algo_name]

def hv_convergenve(algorithm, upp_bound, n_simulations=30):
    runs = []
    for n in tqdm(range(n_simulations)):
        algo = get_algorithm_instance(algorithm)
        algo.run()
        hv = compute_hv(algo, upp_bound)
        runs.append(hv)
    return np.mean(runs, axis=0)

fig, axis= plt.subplots(nrows=2, ncols=3, figsize=(20, 8))
axis = axis.ravel()

reference_point = [0.0, 0.0, 0.0]

smpso_hv_values = []
ompso_hv_values = []
nsgaii_hv_values = []
speaii_hv_values = []
moead_hv_values = []
ibea_hv_values = []

hv_vals = [smpso_hv_values, ompso_hv_values, 
           nsgaii_hv_values, speaii_hv_values,
           moead_hv_values, ibea_hv_values]

algos = ['smpso', 'omopso', 'nsgaii', 'spea2', 'moead', 'ibea']
for idx, (alg, ax) in enumerate(zip(algos, axis)):
    hv_vals[idx].extend(hv_convergenve(algorithm=alg, upp_bound=reference_point, n_simulations=10))
    ax.plot(range(len(hv_vals[idx])), hv_vals[idx], c='b', alpha=0.5)
    ax.set_ylabel('HV')
    ax.set_xlabel('Number of Iterations')
    ax.set_title('Convergence of HV Indicator - {}'.format(alg.upper()))
    
fig.tight_layout()
print('Referecent Point for HV: {}'.format(reference_point))

from sklearn.preprocessing import minmax_scale
fig, axis= plt.subplots(nrows=2, ncols=3, figsize=(15, 7))
axis = axis.ravel()

algos = ['smpso', 'omopso', 'nsgaii', 'spea2', 'moead', 'ibea']
for idx, (alg, ax) in enumerate(zip(algos, axis)):
    hv_values = minmax_scale(hv_vals[idx])
    ax.plot(range(len(hv_values)), hv_values, c='b', alpha=0.5)
    ax.set_ylabel('HV')
    ax.set_xlabel('Number of Iterations')
    ax.set_title('Convergence of HV Indicator - {}'.format(alg.upper()))
    
fig.tight_layout()
fig.savefig('quasi-figs/One-to-One Front Convergence Comparison - HV Indicator.jpg')
print('Referecent Point for HV: {}'.format(reference_point))

"""#### Normalized Front Convergence Comparison
<a id='normal_front_conv'> </a>

[Back to top](#top)
"""

from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import minmax_scale

colors = ['r', 'g', 'b', 'y', 'k', 'c']
algos = ['smpso', 'omopso', 'nsgaii', 'spea2', 'moead', 'ibea']

fig, axis = plt.subplots(nrows=1, ncols=1, figsize=(15, 8))

for alg, c, hv in zip(algos, colors, hv_vals):
    hv_values = minmax_scale(hv)
    axis.plot(range(len(hv_values)), hv_values, c=c, alpha=0.5, label=alg.upper())
    axis.set_ylabel('HV Indicator')
    axis.set_xlabel('Number of Iterations')
    
axis.set_title('Normalized Front Convergence Comparison - HV Indicator')
axis.legend(loc=0)
fig.savefig('quasi-figs/Normalized Front Convergence Comparison - HV Indicator.jpg')

"""#### Spacing and Spreading
<a id='spacing_spread'> </a>

[Back to top](#top)
"""

np.set_printoptions(suppress=True)

def normalized_front(front):
    _, k = front.shape
    normalized_front = copy(front)
    for k_th in range(k):
        normalized_front[:, k_th] = (normalized_front[:, k_th] - \
                                     np.min(normalized_front[:, k_th]))/\
                                    (np.max(normalized_front[:, k_th]) - \
                                     np.min(normalized_front[:, k_th]))
    return normalized_front

class HV(object):
    
    @staticmethod
    def compute(front, upp_bound):
        return HyperVolume(reference_point=upp_bound).compute(front)
    

class Spacing(object):
    
    @staticmethod
    def compute(front):
        n_front = normalized_front(front)
        distances = []
        for p in n_front:
            min_dist = map(lambda x: np.linalg.norm(p - x), n_front)
            min_dist = sorted(min_dist)[1]
            distances.append(min_dist)
        distances = np.array(distances)
        return np.sqrt((1/len(n_front)) * np.sum(np.power((distances - np.mean(distances)), 2)))
    
    
class Spread(object):
    
    @staticmethod
    def compute(front):
        n_front = normalized_front(front)
        distances = []
        for p in n_front:
            min_dist = map(lambda x: np.linalg.norm(p - x), n_front)
            min_dist = sorted(min_dist)[1]
            distances.append(min_dist)
        distances = np.array(distances)
        d_bar = np.mean(distances)
        sum_d = np.sum(np.abs(distances - np.mean(distances)))
        
        _, k = n_front.shape
        d_ek = []
        for k_th in range(k):
            d_ek.append(max(n_front, key=lambda s: s[k_th]))
        
        d_ek_distances = []
        for d in d_ek:
            min_dist = map(lambda x: np.linalg.norm(d - x), n_front)
            min_dist = sorted(min_dist)[1]
            d_ek_distances.append(min_dist)
            
        d_ek = sum(d_ek_distances)
        return (d_ek + sum_d)/(d_ek + len(n_front) * d_bar)

def spacing_convergence(algorithm, n_simulations=30):
    runs = []
    for n in tqdm(range(n_simulations)):
        algo = get_algorithm_instance(algorithm)
        algo.run()
        front = get_raw_front(get_non_dominated_solutions(algo.get_result()), var_dict)
        spacing = Spacing.compute(front)
        runs.append(spacing)
    return runs


def spread_convergence(algorithm, n_simulations=30):
    runs = []
    for n in tqdm(range(n_simulations)):
        algo = get_algorithm_instance(algorithm)
        algo.run()
        front = get_raw_front(get_non_dominated_solutions(algo.get_result()), var_dict)
        spread = Spread.compute(front)
        runs.append(spread)
    return runs


def hypervolume_convergence(algorithm, n_simulations=30):
    runs = []
    for n in tqdm(range(n_simulations)):
        algo = get_algorithm_instance(algorithm)
        algo.run()
        front = get_raw_front(get_non_dominated_solutions(algo.get_result()), var_dict)
        hv = HV.compute(front, reference_point)
        runs.append(hv)
    return runs


def n_solutions_convergence(algorithm, n_simulations=30):
    runs = []
    for n in tqdm(range(n_simulations)):
        algo = get_algorithm_instance(algorithm)
        algo.run()
        runs.append(len(algo.get_results()))
    return runs

algos = ['smpso', 'omopso', 'nsgaii', 'spea2', 'moead', 'ibea']
reference_point = [0.0, 0.0, 0.0]

smpso_spacing_values = []
ompso_spacing_values = []
nsgaii_spacing_values = []
speaii_spacing_values = []
moead_spacing_values = []
ibea_spacing_values = []

spacing_vals = [smpso_spacing_values, ompso_spacing_values, 
                nsgaii_spacing_values, speaii_spacing_values,
                moead_spacing_values, ibea_spacing_values]

smpso_spread_values = []
ompso_spread_values = []
nsgaii_spread_values = []
speaii_spread_values = []
moead_spread_values = []
ibea_spread_values = []

spread_vals = [smpso_spread_values, ompso_spread_values, 
               nsgaii_spread_values, speaii_spread_values,
               moead_spread_values, ibea_spread_values]

smpso_hypervolume_values = []
ompso_hypervolume_values = []
nsgaii_hypervolume_values = []
speaii_hypervolume_values = []
moead_hypervolume_values = []
ibea_hypervolume_values = []

hypervolume_vals = [smpso_hypervolume_values, ompso_hypervolume_values, 
                    nsgaii_hypervolume_values, speaii_hypervolume_values,
                    moead_hypervolume_values, ibea_hypervolume_values]

# smpso_nsolutions_values = []
# ompso_nsolutions_values = []
# nsgaii_nsolutions_values = []
# speaii_nsolutions_values = []
# moead_nsolutions_values = []
# ibea_nsolutions_values = []

# nsolutions_vals = [smpso_nsolutions_values, ompso_nsolutions_values, 
#                    nsgaii_nsolutions_values, speaii_nsolutions_values,
#                    moead_nsolutions_values, ibea_nsolutions_values]


for idx, alg in enumerate(algos):
    spacing_vals[idx].extend(spacing_convergence(algorithm=alg, n_simulations=10))
    spread_vals[idx].extend(spread_convergence(algorithm=alg, n_simulations=10))
    hypervolume_vals[idx].extend(hypervolume_convergence(algorithm=alg, n_simulations=10))
#     nsolutions_vals[idx].extend(n_solutions_convergenge(algorithm=alg, n_simulations=10))

from sklearn.preprocessing import minmax_scale

algos = ['smpso', 'omopso', 'nsgaii', 'spea2', 'moead', 'ibea']

metrics = {
    'HV': hypervolume_vals,
    'S': spacing_vals,
    'Sp': spread_vals,
}

def truncate(f, n):
    '''Truncates/pads a float f to n decimal places without rounding'''
    s = '{}'.format(f)
    if 'e' in s or 'E' in s:
        return '{0:.{1}f}'.format(f, n)
    i, p, d = s.partition('.')
    return '.'.join([i, (d+'0'*n)[:n]])
    

for algo in algos:
    print('\t\t' + algo, end='')
    
print('\n')
for metric in metrics:
    print(metric, end='')
    for vals in metrics[metric]:
        if metric =='HV':
            vals = minmax_scale(vals)
        v = truncate(np.mean(vals), 3) if not truncate(np.mean(vals), 3) == 'nan.000' else '0.000'
        s = truncate(np.std(vals), 3) if not truncate(np.std(vals), 3) == 'nan.000' else '0.000'
        print('\t {}(+-{})'.format(v, s), end='')
    print('\n')

import hashlib
from datetime import datetime

